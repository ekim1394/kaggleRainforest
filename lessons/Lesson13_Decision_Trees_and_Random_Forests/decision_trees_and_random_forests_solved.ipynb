{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Decision Trees and Random Forests\n",
    " \n",
    "_Authors: Arun Ahuja (NYC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand and build decision tree models for classification and regression\n",
    "- Understand the differences between linear and non-linear models\n",
    "- Understand and build random forest models for classification and regression\n",
    "- Know how to extract the most important predictors in a random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Today's dataset: Predicting \"Greenness\" Of Content](#todays-dataset-predicting-greenness-of-content)\n",
    "\t- [What are 'evergreen' sites?](#what-are-evergreen-sites)\n",
    "\t- [Data Dictionary](#data-dictionary)\n",
    "- [Part 1: Explore the dataset](#part--explore-the-dataset)\n",
    "\t- [1.1 Does being a news site affect evergreeness?](#-does-being-a-news-site-affect-evergreeness)\n",
    "\t- [1.2 Does category in general affect evergreeness?](#-does-category-in-general-affect-evergreeness)\n",
    "\t- [1.3 How many articles are there per category?](#-how-many-articles-are-there-per-category)\n",
    "\t- [1.4 Explore additional relationships](#-explore-additional-relationships)\n",
    "\t- [1.5 Can you create any additional features?](#-can-you-create-any-additional-features)\n",
    "- [Introduction to decision trees and random forests](#introduction-to-decision-trees-and-random-forests)\n",
    "\t- [Intuition behind decision trees](#intuition-behind-decision-trees)\n",
    "\t- [What are trees in general?](#what-are-trees-in-general)\n",
    "\t- [How does a computer build a decision tree?](#how-does-a-computer-build-a-decision-tree)\n",
    "- [Interactive demo using housing price](#interactive-demo-using-housing-price)\n",
    "\t- [Comparison to previous models](#comparison-to-previous-models)\n",
    "- [Reading decision trees](#reading-decision-trees)\n",
    "- [Part 2:  Let's Explore Some Decision Trees](#part---lets-explore-some-decision-trees)\n",
    "\t- [2.1 Pre-process dataset](#-pre-process-dataset)\n",
    "\t- [2.2 Build a Decision Tree Model](#-build-a-decision-tree-model)\n",
    "\t- [2.3 Evaluate the Decision Tree Model](#-evaluate-the-decision-tree-model)\n",
    "- [Demo: Overfitting in decision trees](#demo-overfitting-in-decision-trees)\n",
    "\t- [3.1 Check if the model is overfit by checking accuracy on training set vs test set](#-check-if-the-model-is-overfit-by-checking-accuracy-on-training-set-vs-test-set)\n",
    "\t- [3.2 Demo: Bias vs. Variance](#-demo-bias-vs-variance)\n",
    "- [Introduction: Ensembles and random forests](#introduction-ensembles-and-random-forests)\n",
    "- [What is ensembling?](#what-is-ensembling)\n",
    "\t- [Bagging: bootstrap aggregation](#bagging-bootstrap-aggregation)\n",
    "- [Random Forests](#random-forests)\n",
    "- [Codealong: Regression with decision trees and random forests](#codealong-regression-with-decision-trees-and-random-forests)\n",
    "\t- [4.1 Demo: Build a random forest model to predict the evergreeness of a website.](#-demo-build-a-random-forest-model-to-predict-the-evergreeness-of-a-website)\n",
    "\t- [4.2 Tune and update the model with grid search](#-tune-and-update-the-model-with-grid-search)\n",
    "- [Topic review](#topic-review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todays-dataset-predicting-greenness-of-content\"></a>\n",
    "## Today's dataset: Predicting \"EverGreenness\" Of Content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-are-evergreen-sites\"></a>\n",
    "### What are 'evergreen' sites?\n",
    "\n",
    "> #### Evergreen sites are those that are always relevant.  As opposed to breaking news or current events, evergreen websites are relevant no matter the time or season. \n",
    "\n",
    "> #### A sample of URLs is below, where label = 1 are 'evergreen' websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-dictionary\"></a>\n",
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise uses the [Kaggle StumbleUpon Evergreen Classification Challenge](https://www.kaggle.com/c/stumbleupon)\n",
    "\n",
    "This dataset comes from [StumbleUpon](https://www.stumbleupon.com/), a web page recommender. A description of the columns is below:\n",
    "\n",
    "FieldName|Type|Description\n",
    "---------|----|-----------\n",
    "url|string|Url of the webpage to be classified\n",
    "title|string|Title of the article\n",
    "body|string|Body text of article\n",
    "urlid|integer| StumbleUpon's unique identifier for each url\n",
    "boilerplate|json|Boilerplate text\n",
    "alchemy_category|string|Alchemy category (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "alchemy_category_score|double|Alchemy category score (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "avglinksize| double|Average number of words in each link\n",
    "commonlinkratio_1|double|# of links sharing at least 1 word with 1 other links / # of links\n",
    "commonlinkratio_2|double|# of links sharing at least 1 word with 2 other links / # of links\n",
    "commonlinkratio_3|double|# of links sharing at least 1 word with 3 other links / # of links\n",
    "commonlinkratio_4|double|# of links sharing at least 1 word with 4 other links / # of links\n",
    "compression_ratio|double|Compression achieved on this page via gzip (measure of redundancy)\n",
    "embed_ratio|double|Count of number of <embed> usage\n",
    "frameBased|integer (0 or 1)|A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "frameTagRatio|double|Ratio of iframe markups over total number of markups\n",
    "hasDomainLink|integer (0 or 1)|True (1) if it contains an <a> with an url with domain\n",
    "html_ratio|double|Ratio of tags vs text in the page\n",
    "image_ratio|double|Ratio of <img> tags vs text in the page\n",
    "is_news|integer (0 or 1) | True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "lengthyLinkDomain| integer (0 or 1)|True (1) if at least 3 <a> 's text contains more than 30 alphanumeric characters\n",
    "linkwordscore|double|Percentage of words on the page that are in hyperlink's text\n",
    "news_front_page| integer (0 or 1)|True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "non_markup_alphanum_characters|integer| Page's text's number of alphanumeric characters\n",
    "numberOfLinks|integer Number of <a>|markups\n",
    "numwords_in_url| double|Number of words in url\n",
    "parametrizedLinkRatio|double|A link is parametrized if it's url contains parameters or has an attached onClick event\n",
    "spelling_errors_ratio|double|Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "label|integer (0 or 1)|User-determined label. Either evergreen (1) or non-evergreen (0); available for train.tsv only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import model_selection\n",
    "from sklearn import grid_search\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"../../dataset/stumbleupon.tsv\", sep='\\t')\n",
    "\n",
    "# Split `boilerplate` column\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "# Check head\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boilerplate[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check URLs and their evergreen labels\n",
    "data[['url', 'label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part--explore-the-dataset\"></a>\n",
    "## Part 1: Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will revisit the data science workflow in order to explore the dataset and determine important characteristics for \"evergreen\" websites.\n",
    "\n",
    "1. Prior to looking at the available data, brainstorm 3 - 5 characteristics that would be useful for predicting evergreen websites.\n",
    "2. After looking at the dataset, can you model or quantify any of the characteristics you wanted?\n",
    "  - For instance, if you believe high-image content websites are likely to be evergreen, then how would you build a feature that represents high image content?\n",
    "  - Or if you believe weather content ISN'T likely to be evergreen, then how would you build a feature to reflect that?            \n",
    "3. Does being a news site affect \"evergreen-ness\"? Compute or plot the percent of evergreen news sites.\n",
    "4. Does category in general affect evergreen-ness? Plot the rate of evergreen sites for all Alchemy categories.\n",
    "5. How many articles are there per category?\n",
    "6. Create a feature for the title containing \"recipe\". Is the % of evergreen websites higher or lower on pages that have \"recipe\" in the the title?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-does-being-a-news-site-affect-evergreeness\"></a>\n",
    "### 1.1 Does being a news site affect evergreeness? \n",
    "Compute or plot the percentage of news related evergreen sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using groupby()\n",
    "data.groupby('is_news').label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_news'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_news'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-does-category-in-general-affect-evergreeness\"></a>\n",
    "### 1.2 Does category in general affect evergreeness? \n",
    "Plot the rate of evergreen sites for all Alchemy categories.\n",
    "\n",
    "Note: The website categories were collected using AlchemyAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using groupby()\n",
    "data.groupby('alchemy_category').label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a plot\n",
    "data.groupby('alchemy_category').label.mean().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-how-many-articles-are-there-per-category\"></a>\n",
    "### 1.3 How many articles are there per category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using groupby()\n",
    "data.alchemy_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('alchemy_category').label.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a plot\n",
    "data.alchemy_category.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"-explore-additional-relationships\"></a>\n",
    "### 1.4 Explore additional relationships\n",
    "Are there any other relationships you brainstormed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.image_ratio.plot(kind='box', by='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot(column='image_ratio', by='label')\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.image_ratio.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-can-you-create-any-additional-features\"></a>\n",
    "### 1.5 Can you create any additional features?\n",
    "Create a feature that indicates whether the title contains the word 'recipe'. Is the percent of evegreen websites higher or lower on pages that have recipe in the the title?\n",
    "\n",
    "Hint: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if title contains the word 'recipe'\n",
    "data['recipe_in_title'] = data['title'].str.contains('recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using groupby()\n",
    "data.groupby(['recipe_in_title'])[['label']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a plot\n",
    "sns.factorplot(x='recipe_in_title', y='label', kind='bar', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction-to-decision-trees-and-random-forests\"></a>\n",
    "## Introduction to decision trees and random forests\n",
    "Objective: Understand decision tree models for classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intuition-behind-decision-trees\"></a>\n",
    "### Intuition behind decision trees\n",
    "\n",
    "Decision trees are like the game “20 questions”.  They make decision by answering a series of questions, most often binary questions (yes or no). \n",
    "\n",
    "For example, if we want to classify between squirrels, cats, and dogs, we will collect data about weight. If the weight is less than two pounds, we tend to see squirrels in our data. If the weight is greater than 2 pounds and less than 20 pounds, we may guess dog or cat. If the wieght is greater than 2 pounds and greater than 20 pounds, we will guess dog.\n",
    "\n",
    "We want the smallest set of questions to get to the right answer.\n",
    "\n",
    "Each questions should reduce the search space as much as possible.\n",
    "\n",
    "Decision trees work for both classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-are-trees-in-general\"></a>\n",
    "### What are trees in general?\n",
    "\n",
    "- Trees are a data structure made up of nodes and branches.\n",
    "- Each node typically has two or more branches that connect it to its children.\n",
    "- Each child is another node in the tree and contains its own subtree.  \n",
    "- Nodes without any children are known as leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the case of Decision Trees**\n",
    "- A decision tree contains a question at every node.\n",
    "- Depending upon the answer to the question, we proceed down the left or right branch of the tree and ask another question.\n",
    "- Once we don’t have any more questions (at the leaf nodes), we make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-does-a-computer-build-a-decision-tree\"></a>\n",
    "### How does a computer build a decision tree?\n",
    "\n",
    "**Ideal approach**: Consider every possible partition of the feature space (computationally infeasible)\n",
    "**\"Good enough\" approach:** recursive binary splitting\n",
    "1. Begin at the top of the tree.\n",
    "2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint such that the resulting tree has the lowest possible mean squared error (MSE). Make that split.\n",
    "3. Examine the two resulting regions, and again make a **single split** (in one of the regions) to minimize the MSE.\n",
    "4.Keep repeating step 3 until a **stopping criterion** is met:\n",
    "- maximum tree depth (maximum number of splits required to arrive at a leaf)\n",
    "- minimum number of observations in a leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"interactive-demo-using-housing-price\"></a>\n",
    "## Interactive demo using housing price\n",
    "\n",
    "Let’s suppose we want to predict if a house is in SF or NYC.  \n",
    "\n",
    "Follow along here: http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison-to-previous-models\"></a>\n",
    "### Comparison to previous models\n",
    "\n",
    "Pros:\n",
    "- Decision trees are non-linear (rectilinear), an advantage over logistic regression.\n",
    "  - A linear model is one in which a change in an input variable has a constant change on the output variable.\n",
    "- Can find interactions between variables.\n",
    "- Can take continuous variables and treat them as ordered discrete/ordinal variables.\n",
    "- Works for both regression and classification.\n",
    "- Does not need scaling.\n",
    "- Not greatly affected by outliers.\n",
    "- Implicitly performs feature selection.\n",
    "\n",
    "Cons:\n",
    "- A bit more difficult to interpret.\n",
    "- Does not give you something like a p-value.\n",
    "  - Although, there is a way to check overall feature importance.\n",
    "- Does not fit linear relationships very well.\n",
    "\n",
    "\n",
    "    Linear vs. non-linear classification models\n",
    "![Linear vs Non-linear](assets/images/linear_vs_non_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example consider the relationship between years of education and salary\n",
    "\n",
    "- In a linear model, the increase in salary from 10 to 15 years of education would be the same as the increase in salary from 15 to 20 years of education.  \n",
    "\n",
    "- In a non-linear model, salary can change dramatically for years 0-15 and negligibly from years 15-20.\n",
    "\n",
    "Trees automatically contain interaction of features, since each question is dependent on the last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we quantify how well we segragate positive and negative groups?**\n",
    "\n",
    "1. Pick a feature at random\n",
    "2. Select a cut-off to form two subgroup\n",
    "3. For each subgroup predict the most common class for a selected subgroup and measure the classification error\n",
    "4. Continue until we find the best cut-off\n",
    "5. Now repeat on the two subgroups until we are satisfied with the performance or run out of points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-decision-trees\"></a>\n",
    "## Reading decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.recipe_in_title > .5].label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build a sample tree for our evergreen prediction problem.  Assume our features are whether the article contains a recipe, the image ratio, the html ratio.\n",
    "First, let’s choose the feature that gives us the highest purity, the recipe feature.\n",
    "![](assets/images/single-node-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The top line shows the decision rule.\n",
    "\n",
    "- gini refers to **Gini Impurity**: how often a randomly chosen element in the set would be incorrectly labeled if we chose a label at random.\n",
    "- Samples refers to how many data points are in the group before splitting on the decision rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take each side of the tree and repeat the process.\n",
    "\n",
    "![](assets/images/depth-2-tree.png)\n",
    "\n",
    "We can continue this process until we have asked as many questions as we want or until our leaf nodes are completely pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we make prediction?**\n",
    "\n",
    "Predictions are made by answering each of the questions.\n",
    "\n",
    "Once we reach a leaf node, our prediction is made by taking the majority label of the training samples that fulfill the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**knowledge check**\n",
    "\n",
    "In the tree above ..\n",
    "\n",
    "1. Which node would an article that does not have the word \"recipe\" and has a lot of images fall into?\n",
    "2. What is the probability that this article is evergreen?\n",
    "\n",
    "<!--\n",
    "ANSWER:\n",
    "\n",
    "    In our sample tree, if we want to classify a new article, ask:\n",
    "Does the article contain the word recipe?\n",
    "If it doesn’t, does the article have a lot of images?\n",
    "If it does, then 313 / 943 article are evergreen.\n",
    "So we can assign a 0.33 probability for evergreen sites.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codealong: decision trees in scikit-learn\n",
    "Objective: Build decision tree models for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part---lets-explore-some-decision-trees\"></a>\n",
    "## Part 2:  Let's Explore Some Decision Trees\n",
    "\n",
    "Demo: Build a decision tree model to predict the \"evergreeness\" of a given website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-pre-process-dataset\"></a>\n",
    "### 2.1 Pre-process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dtypes and missing values\n",
    "pd.DataFrame({'dtypes': data.dtypes, 'missing':data.isnull().sum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Create dummy variables for alchemy_category\n",
    "data = (\n",
    "    data\n",
    "    .join(pd.get_dummies(data['alchemy_category'], prefix='alchemy_cat'))\n",
    "    .drop(['alchemy_category'], axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-build-a-decision-tree-model\"></a>\n",
    "### 2.2 Build a Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refresh lambda functions\n",
    "filter(lambda x: len(x) < 5, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[1].startswith('url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Set features to use\n",
    "features = ['image_ratio', 'html_ratio', 'recipe_in_title'] + \\\n",
    "            filter(lambda x: x.startswith('alchemy_cat_'), data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set target variable name\n",
    "target = 'label'\n",
    "\n",
    "# Set X and y\n",
    "X = data[features]\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create separate training and test sets with 60/40 train/test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate model using default params\n",
    "tm = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on training set\n",
    "tm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy of model on test set\n",
    "print \"Accuracy: %0.3f\" % tm.score(X_test, y_test)\n",
    "\n",
    "# Evaluate ROC AUC score of model on test set\n",
    "print 'ROC AUC: %0.3f' % metrics.roc_auc_score(y_test, tm.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-evaluate-the-decision-tree-model\"></a>\n",
    "### 2.3 Evaluate the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrix on test set\n",
    "y_pred = tm.predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm_normalized, annot=True)\n",
    "ax.set_ylabel('True')\n",
    "ax.set_xlabel('Pred')\n",
    "plt.show()\n",
    "\n",
    "print \"Confusion Matrix:\"\n",
    "print cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"demo-overfitting-in-decision-trees\"></a>\n",
    "## Demo: Overfitting in decision trees\n",
    "Objective: Understand the differences between linear and non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees tend to be weak models because they can easily memorize or overfit to a dataset.\n",
    "\n",
    "A model is overfit when it memorizes or bends to a few specific data points rather than picking up general trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unconstrained decision tree can learn an extreme tree (e.g. one feature for each word in a news article).\n",
    "\n",
    "We can limit our decision trees using a few methods.\n",
    "Limiting the number of questions (nodes) a tree can have).\n",
    "Limiting the number of samples in the leaf nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "hyperparameters = [(8, 1), (8, 5)]\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data[:, :2]  # we only take the first two features. We could\n",
    "# avoid this ugly slicing by using a two-dim dataset\n",
    "y_iris = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for max_depth, min_samples_leaf in hyperparameters:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=max_depth, \n",
    "                                      min_samples_leaf=min_samples_leaf)\n",
    "    clf.fit(X_iris, y_iris)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X_iris[:, 0].min() - 1, X_iris[:, 0].max() + 1\n",
    "    y_min, y_max = X_iris[:, 1].min() - 1, X_iris[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (max_depth = %i, min_samples_leaf = %i)\"\n",
    "              % (max_depth, min_samples_leaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**knowledge check**\n",
    "\n",
    "Looking at the above chart it should be clear that the top graph shows a Decision Tree that is over-fitting. Out of max_depth and min_samples_leaf, which parameter do you think would be the best to use in this case to control the over-fit?\n",
    "\n",
    "<!--\n",
    "ANSWER:\n",
    "\n",
    "There are two important patterns in the graphs above. You can see small clusters that contain only one point, this is very likely to be noise. You can also see You can also see that the interaction between our two variables is important as we need to approximate diagnol lines to separate the red and green classes. \n",
    "\n",
    "We can control overfitting either by reducing that max_depth or by increasing the number of samples required to remain in a leaf.\n",
    "\n",
    "In this case we don't want to get rid of the diagnols, so we want to keep max_depth at a reasonable level, but we can find noisy cases where we focus in one one point. Those noisy cases can be avoided by increasing the min_samples_leaf parameter.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-check-if-the-model-is-overfit-by-checking-accuracy-on-training-set-vs-test-set\"></a>\n",
    "### 3.1 Check if the model is overfit by checking accuracy on training set vs test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on train set\n",
    "print \"Accuracy: %0.3f\" % tm.score(X_train, y_train)\n",
    "\n",
    "# Evaluate model on test set\n",
    "print \"Accuracy: %0.3f\" % tm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-demo-bias-vs-variance\"></a>\n",
    "### 3.2 Demo: Bias vs. Variance\n",
    "Control for overfitting in the decision model by adjusting the maximum number of questions (max_depth) or the minimum number of records in each final node (min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model using default params\n",
    "tm = tree.DecisionTreeClassifier(max_depth=7, min_samples_leaf=5)\n",
    "\n",
    "# Train model on training set\n",
    "tm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on train set\n",
    "print \"Accuracy: %0.3f\" % tm.score(X_train, y_train)\n",
    "\n",
    "# Evaluate model on test set\n",
    "print \"Accuracy: %0.3f\" % tm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction-ensembles-and-random-forests\"></a>\n",
    "## Introduction: Ensembles and random forests\n",
    "Objective: Understand random forest models for classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend that instead of building a single model to solve a binary classification problem, you created **five independent models**, and each model was correct about 70% of the time. If you combined these models into an \"ensemble\" and used their majority vote as a prediction, how often would the ensemble be correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-ensembling\"></a>\n",
    "## What is ensembling?\n",
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model.\n",
    "- **Regression**: take the average of the predictions\n",
    "- **Classification**: take a vote and use the most common prediction, or take the average of the predicted probabilities\n",
    "\n",
    "For ensembling to work well, the models must have the following characteristics:\n",
    "- **Accurate**: they outperform the null model\n",
    "- **Independent**: their predictions are generated using different processes\n",
    "\n",
    "    **The big idea**: If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when averaging the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/crowdflower_ensembling.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bagging-bootstrap-aggregation\"></a>\n",
    "<a id=\"bagging-bootstrap-aggregation\"></a>\n",
    "<a id=\"bagging-bootstrap-aggregation\"></a>\n",
    "### Bagging: bootstrap aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary weakness of **decision trees** is that they don't tend to have the best predictive accuracy. This is partially due to **high variance**, meaning that different splits in the training data can lead to very different trees.\n",
    "\n",
    "**Bagging** is a general purpose procedure for reducing the variance of a machine learning method, but is particularly useful for decision trees. Bagging is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n",
    "\n",
    "What is a **bootstrap sample**? A random sample with replacement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does bagging work (for decision trees)?**\n",
    "1. Grow B trees using B bootstrap samples from the training data.\n",
    "2. Train each tree on its bootstrap sample and make predictions.\n",
    "3. Combine the predictions:\n",
    " - Average the predictions for regression trees\n",
    " - Take a vote for classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"random-forests\"></a>\n",
    "<a id=\"random-forests\"></a>\n",
    "<a id=\"random-forests\"></a>\n",
    "<a id=\"random-forests\"></a>\n",
    "<a id=\"random-forests\"></a>\n",
    "## Random Forests\n",
    "Random Forests is a slight variation of bagged trees that has even better performance:\n",
    "- Exactly like bagging, we create an ensemble of decision trees using bootstrapped samples of the training set.\n",
    "- However, when building each tree, each time a split is considered, a random sample of m features is chosen as split candidates from the full set of p features. The split is only allowed to use one of those m features.\n",
    "- A new random sample of features is chosen for every single tree at every single split.\n",
    "- For classification, m is typically chosen to be the square root of p.\n",
    "- For regression, m is typically chosen to be somewhere between p/3 and p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the point?**\n",
    "\n",
    "Suppose there is **one very strong feature** in the data set. When using bagged trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are **highly correlated**.\n",
    "- Averaging highly correlated quantities does not significantly reduce variance (which is the entire goal of bagging).\n",
    "- By randomly leaving out candidate features from each split, **Random Forests \"decorrelates\" the trees**, such that the averaging process can reduce the variance of the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"codealong-regression-with-decision-trees-and-random-forests\"></a>\n",
    "<a id=\"codealong-regression-with-decision-trees-and-random-forests\"></a>\n",
    "<a id=\"codealong-regression-with-decision-trees-and-random-forests\"></a>\n",
    "<a id=\"codealong-regression-with-decision-trees-and-random-forests\"></a>\n",
    "<a id=\"codealong-regression-with-decision-trees-and-random-forests\"></a>\n",
    "## Codealong: Regression with decision trees and random forests\n",
    "Objective: Build random forest models for classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forest models are one of the most widespread classifiers used.\n",
    "- They are relatively simple to use and help avoid overfitting.\n",
    "- Random Forests are an ensemble or collection of individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training an Random Forest**\n",
    "\n",
    "Training a random forest model involves training many decision tree models.\n",
    "\n",
    "Since decision trees overfit easily, we use many decision trees together and randomize the way they are created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/images/tree_ensemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Algorithm**\n",
    "\n",
    "1. Take a bootstrap sample of the dataset. (Sample random rows)\n",
    "2. Train a decision tree on the bootstrap sample.  For each split/feature selection, only evaluate a limited number of features to find the best one.\n",
    "3. Repeat this for N trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I make predictions with a collection of trees?**\n",
    "- Predictions for a random forest model come from each decision tree.\n",
    "- Make an individual prediction with each decision tree.\n",
    "- Combine the individual predictions and take the majority vote or average in the case of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does this work?**\n",
    "- A tree with a large depth is relatively low bias and high variance\n",
    "- When we average the predictions of the trees we reduce the variance\n",
    "- The amazing part is that we don't raise the bias unless the trees are correlated\n",
    "- We get a sharper estimation of the predictions, just like taking the mean of a large sample vs a small sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real world application**\n",
    "\n",
    "Ensembling is one of the secret sauces to Kaggle competitions.\n",
    "\n",
    "Ensembling is not only limited to trees, you can use any other model or a mix of models to ensemble, provided that their predictions are generally uncorrelated and are better than random. \n",
    "\n",
    "This means that you should mix models that work differently from each other, such as linear vs non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**knowledge check**\n",
    "\n",
    "Can someone describe in words what they would do to create an ensemble of models in scikit-learn?\n",
    "\n",
    "<!--\n",
    "ANSWER:\n",
    "    1. Run fit on each model\n",
    "    2. Run predict on each model\n",
    "    3. Combine the predictions as columns in a dataframe\n",
    "    4. Average the columns horizontally in the case of regression\n",
    "      - Select the most common class in the case of classification\n",
    "    5. Use the resulting column as your new predictions\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codealong: Evaluate random forests using cross-validation\n",
    "Objective: Know how to extract the most important predictors in a random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Random Forests\n",
    "\n",
    "<a id=\"-demo-build-a-random-forest-model-to-predict-the-evergreeness-of-a-website\"></a>\n",
    "### 4.1 Demo: Build a random forest model to predict the evergreeness of a website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "rf = ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on training set\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on train set\n",
    "print \"Accuracy: %0.3f\" % rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "print \"Accuracy: %0.3f\" % rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"-tune-and-update-the-model-with-grid-search\"></a>\n",
    "### 4.2 Tune and update the model with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set list of values to grid search over\n",
    "n = [1, 2, 3, 10, 20, 30, 100, 200, 300]\n",
    "params = {'n_estimators': n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search using list of values\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(min_samples_leaf=30),\n",
    "    param_grid=params)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best value to use\n",
    "print \"Best Params:\"\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get improvement\n",
    "print \"Accuracy of current model: %0.3f\" % rf.score(X_test, y_test)\n",
    "print \"Accuracy using best param: %0.3f\" % gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores\n",
    "plt.plot(n, [s[1] for s in gs.grid_scores_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current model params\n",
    "print rf\n",
    "print \"Accuracy of current model: %0.3f\" % rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model params\n",
    "rf.set_params(n_estimators=gs.best_params_['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain model on new params\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated model params\n",
    "print rf\n",
    "print \"Accuracy of updated model: %0.3f\" % rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importances for all features\n",
    "features = X.columns\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "features_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\n",
    "features_df.sort_values('Importance Score', inplace=True, ascending=False)\n",
    "\n",
    "sns.barplot(y='Features', x='Importance Score', data=features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to combine dummy features importances\n",
    "def combine_dummies(dummy_prefixes):\n",
    "    for p in dummy_prefixes:\n",
    "        sub_keys = filter(lambda x: x.startswith(p), feature_dict)\n",
    "        sub_keys_sum = sum([feature_dict[x] for x in sub_keys])\n",
    "        for k in sub_keys: feature_dict.pop(k)\n",
    "        feature_dict[p] = sub_keys_sum\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importances with dummy features combined\n",
    "feature_names = X.columns\n",
    "feature_importances = rf.feature_importances_\n",
    "feature_dummy_prefixes = ['alchemy_cat_']\n",
    "\n",
    "feature_dict = dict(zip(feature_names, feature_importances))\n",
    "feature_dict = combine_dummies(feature_dummy_prefixes)\n",
    "\n",
    "features_df = pd.DataFrame(feature_dict.items(), columns=['Features', 'Importance Score'])\n",
    "features_df.sort_values('Importance Score', inplace=True, ascending=False)\n",
    "\n",
    "sns.barplot(y='Features', x='Importance Score', data=features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic-review\"></a>\n",
    "<a id=\"topic-review\"></a>\n",
    "<a id=\"topic-review\"></a>\n",
    "<a id=\"topic-review\"></a>\n",
    "## Topic review\n",
    "\n",
    "What are decision trees?\n",
    "\n",
    "<!--\n",
    "- A machine learning algorithm that partitions the data into blocks by asking whether a specific data point is greater than or less than a particular value. We can then later use the means, medians, or most common class of the partition to make predictions on both regression and classification problems.\n",
    "-->\n",
    "\n",
    "What does training involve?\n",
    "\n",
    "<!--\n",
    "- To train a decision tree we pick a variable to split and see how well it discriminates our data. We try several variables to see which creates the best split based on a metric such as gini impurity or entropy. We then do split the data further until we are satisfied the our tree can explain our data.\n",
    "-->\n",
    "\n",
    "What are some common problems with decision trees?\n",
    "\n",
    "<!--\n",
    "- They are very likely to overfit if you grow a large enough tree and have trouble dealing with linear relationships or specific types of interactions between between variables. \n",
    "-->\n",
    "\n",
    "What are random forests?\n",
    "\n",
    "<!--\n",
    "- Random forests, in essence, train many trees and average their results. This averaging process is called ensembling. By averaging the trees we trained on we balance out the tendency of trees to overfit by giving a more reserved guess on every data point. This has the astounding effect of not greatly impacting the bias of our algorithms, but greatly reducing the variance.\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
