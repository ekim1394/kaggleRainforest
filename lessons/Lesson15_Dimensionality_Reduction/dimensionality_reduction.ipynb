{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Dimensionality Reduction\n",
    " \n",
    "_Authors: Kiefer Katovich (SF), Alexander Combs (NYC), Alexander Egorenkov (DC)_\n",
    " \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "\n",
    "- Describe high dimensionality\n",
    "- Explain dimensionality reduction\n",
    "- Explain the curse of dimensionality\n",
    "- Describe the pros and cons of increasing and decreasing dimensonality\n",
    "- Describe kernel methods and what they are used for in data science.\n",
    "- Describe what PCA does and what it is used for in data science.\n",
    "- Apply kernel methods to the mammals data\n",
    "- Apply PCA to the titanic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [What is high dimensionality?](#what-is-high-dimensionality)\n",
    "- [Problems related to high dimensionality:](#problems-related-to-high-dimensionality)\n",
    "\t- [High memory requirements and Long model runtime](#high-memory-requirements-and-long-model-runtime)\n",
    "\t- [Non-informative features and multi-collinearity](#non-informative-features-and-multi-collinearity)\n",
    "\t- [The curse of dimensionality](#the-curse-of-dimensionality)\n",
    "\t- [Example - one dimension](#example---one-dimension)\n",
    "\t- [Example - two dimensions](#example---two-dimensions)\n",
    "\t- [Example - three dimensions](#example---three-dimensions)\n",
    "- [Exceptions to the curse of dimensionality:](#exceptions-to-the-curse-of-dimensionality)\n",
    "- [Dealing with high dimensionality](#dealing-with-high-dimensionality)\n",
    "- [Feature Selection](#feature-selection)\n",
    "\t- [Selection by hand](#selection-by-hand)\n",
    "\t- [LASSO](#lasso)\n",
    "\t- [RandomForests](#randomforests)\n",
    "- [Feature Selection Demo: Needle in a haystack](#feature-selection-demo-needle-in-a-haystack)\n",
    "- [Feature Extraction](#feature-extraction)\n",
    "\t- [Feature engineering](#feature-engineering)\n",
    "\t- [Kernels](#kernels)\n",
    "\t- [Unsupervised dimensionality reduction and manifold learning algorithms](#unsupervised-dimensionality-reduction-and-manifold-learning-algorithms)\n",
    "- [Kernels Demo](#kernels-demo)\n",
    "- [Dimensionality Reduction Demo](#dimensionality-reduction-demo)\n",
    "\t- [The Process of PCA](#the-process-of-pca)\n",
    "\t- [Principal Components](#principal-components)\n",
    "\t- [Why would we want to do PCA?](#why-would-we-want-to-do-pca)\n",
    "- [Other Applications](#other-applications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-high-dimensionality\"></a>\n",
    "## What is high dimensionality?\n",
    "\n",
    "High dimensionality occurs when we are attempting to proces many features with a model.\n",
    "\n",
    "A special case is when there are more feature than there are observations (p>n):\n",
    "- Traditional models often can't be estimated in this case\n",
    "- Machine learning models find a way to remove features of reduce redundant information\n",
    "\n",
    "Even when there are fewer features than there are observations, high dimensionality can hinder model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problems-related-to-high-dimensionality\"></a>\n",
    "## Problems related to high dimensionality:\n",
    "- High memory requirements\n",
    "- Long model runtime\n",
    "- Non-informative features\n",
    "- Multi-collinearity\n",
    "- Hard to visualize\n",
    "- The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"high-memory-requirements-and-long-model-runtime\"></a>\n",
    "### High memory requirements and Long model runtime\n",
    "\n",
    "When there are many feature to process. many models take a long time to compute.\n",
    "- For models like KNN slow down propotional to the number of features because we have to compute distance in more and more dimensions.\n",
    "- Models like Linear Regression take a longer time to train because there is more information to process during optimization.\n",
    "- Tree based models have to consider more potential partitions of the data to account for every possible feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"non-informative-features-and-multi-collinearity\"></a>\n",
    "### Non-informative features and multi-collinearity\n",
    "\n",
    "Typically when we have many features, they are not all useful.\n",
    "- Ideally, every feature we add to our model carries new information and helps use make better predictions.\n",
    "- In many cases, our data is redundant and uses highly correlated information that describes similar effects.\n",
    "- In other cases, many of our feature are entirely irrelevant and do not contain any predictive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-curse-of-dimensionality\"></a>\n",
    "### The curse of dimensionality\n",
    "As we **increase the number of dimensions (feature space)**, we effectively **increase the \"empty space\"** that our samples \"live in\".\n",
    "\n",
    "- This is especially problematic for algorithms like KNN that look for the nearest neighbors because it becomes difficult to compute which neighbor is actually nearest.\n",
    "- This is least problematic for algorithms like Naive Bayes that assume compelete independence of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example---one-dimension\"></a>\n",
    "### Example - one dimension\n",
    "![](./assets/images/1-d-example.png)\n",
    "- Taken from  http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example---two-dimensions\"></a>\n",
    "### Example - two dimensions\n",
    "![](./assets/images/2-d-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example---three-dimensions\"></a>\n",
    "### Example - three dimensions\n",
    "![](./assets/images/3-d-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exceptions-to-the-curse-of-dimensionality\"></a>\n",
    "## Exceptions to the curse of dimensionality:\n",
    "- The blessing of non-uniformity\n",
    "There is a corollary to the curse of dimensionality, however, known as the \"blessing of non-uniformity\"\n",
    "> \"In most applications examples are not spread uniformly throughout\n",
    "the instance space, but are concentrated on or near\n",
    "a **lower-dimensional manifold**. For example, k-nearest neighbor\n",
    "works quite well for handwritten digit recognition even\n",
    "though images of digits have one dimension per pixel, because\n",
    "the space of digit images is much smaller than the\n",
    "space of all possible images.\" -Pedro Domingos\n",
    "- Some classifers generalize well in high dimensions\n",
    ">  \"**Classifiers that tend to model non-linear decision boundaries very accurately (e.g. neural networks, KNN classifiers, decision trees) do not generalize well and are prone to overfitting.** Therefore, the dimensionality should be kept relatively low when these classifiers are used. If a classifier is used that generalizes easily (e.g. naive Bayesian, linear classifier), then the number of used features can be higher since the classifier itself is less expressive.\"\n",
    "- We can create seperability in higher dimensions that does not exist in low dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With certain classifiers (linear) - going from a dimensionality that is too low doesn't allow us to effectively create a separating hyperplane. For example in our dogs and cats example, in 1d and 2d, we couldn't insert a separating plane to classify them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In 3D, we can**\n",
    "![](./assets/images/3d-hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But if we go too far....we can overfit our data as a consequence of the curse of dimensionality\n",
    "**\n",
    "![](./assets/images/projected-hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A more tangible example is replacing a variables with one or more polynomial functions.**\n",
    "\n",
    "Below we have a chart that that is not linearly seperable\n",
    "\n",
    "Can you think of a way to make it linearly seperable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('fivethirtyeight') \n",
    "plt.rcParams['figure.figsize'] = (14,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure()\n",
    "#plt.subplot(2, 2, 1, aspect='equal')\n",
    "plt.title(\"Original space\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.plot(X[reds, 0], X[reds, 1], \"ro\")\n",
    "plt.plot(X[blues, 0], X[blues, 1], \"bo\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dealing-with-high-dimensionality\"></a>\n",
    "## Dealing with high dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've established that too many irrelevant features or redundant features can be a problem for our algorithms. What can we do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-selection\"></a>\n",
    "## Feature Selection\n",
    "\n",
    "The process of removing irrelevant features is called feature selection. \n",
    "\n",
    "For the most part, feature select just gets rid of variables that do not correlate with out out come variable.\n",
    "\n",
    "This is surprisingly difficult because:\n",
    "- Correlation is a linear measure of associated and we need to account for non-linearlity.\n",
    "- Something the combination of features is relevant even though each individual feature is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common approaches to feature selection are:\n",
    "- Selection by hand\n",
    "- LASSO\n",
    "- RandomForests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"selection-by-hand\"></a>\n",
    "### Selection by hand\n",
    "\n",
    "We should always aim to include all relevant features and no irrelevant features in our model, but this is often difficult.\n",
    "\n",
    "We may not have enough domain knowledge to be able to intelligently select features.\n",
    "\n",
    "The data may be too large to scan through and we are forced to select features using an automated processes.\n",
    "\n",
    "This approach is ideal, but often takes too much time are is impossible with certain types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lasso\"></a>\n",
    "### LASSO\n",
    "LASSO, also known as L1 regularization, is a variation of linear and logistic regression that penalizes the magnitude of coefficients much like Ridge regression.\n",
    "\n",
    "Any cofficients that are too large relative to the amount of error that they explain in the model are shrunk.\n",
    "\n",
    "Unlike Ridge regression, LASSO will bring model coefficients all the way to zero effectively removing them from the linear regression.\n",
    "\n",
    "LASSO is a significant improvement over looking at correlation because LASSO can account for many features at the same time.\n",
    "\n",
    "LASSO is a frequently used approach to feature selection, but it has several limitations.\n",
    "\n",
    "If we have too many irrelevant features, LASSO may not remove all of them. In other words, LASSO itself falls victim to the curse of dimensionality.\n",
    "\n",
    "A high degree of multi-colinearity can also be problematic. Although, typically, if there are several multi-colinear variables, LASSO will just pick one for the model.\n",
    "\n",
    "The other large weakness of LASSO is that it only looks at linear relationships between variables.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/linear_model.html#lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"randomforests\"></a>\n",
    "### RandomForests\n",
    "\n",
    "It may not be immediately obvious, but random forests have built in feature selection.\n",
    "\n",
    "Remember that random forests build a multitude of decision trees on random samples of data and **random samples of features**.\n",
    "\n",
    "Decision trees look for features that do the best job of partitioning our data to reduce MSE or Gini Impurity.\n",
    "\n",
    "It turns out that by looking at random features and selecting the ones that have the most explanatory power, we select the most important features for the random forest.\n",
    "\n",
    "By looking at feature importances for a random forest, we can exclude the most irrelevant features.\n",
    "\n",
    "Random forests make one major improvement over LASSO, which is the ability to handle non-linear relationships.\n",
    "\n",
    "However, random forests typically perform worse in high dimensions and don't do as well with linear relationships as LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-selection-demo-needle-in-a-haystack\"></a>\n",
    "## Feature Selection Demo: Needle in a haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is simple, you need to use LASSO to find the irrelevant feature. \n",
    "\n",
    "I've created a simulated dataset that offers very little visual cue as to which features are important. Among the twelve features, there is precisely 1 that has no effect on the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the haystack data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../dataset/haystack.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feel free to do some exploratory data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import LASSO for use**\n",
    "\n",
    "Documentation can be found here: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create X feature matric and y outcome**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the LASSO regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View LASSO coefficients and identify which feature is irrelevant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-extraction\"></a>\n",
    "## Feature Extraction\n",
    "Whereas feature selection attempts to discover relevant subsets of the original data, feature extraction combines key features of a potentially correlated dataset resulting in a new  more linear non-correlated dataset with fewer features.\n",
    "\n",
    "Common approaches to feature extraction are:\n",
    "- Feature engineering\n",
    "- Kernels\n",
    "- Unsupervised dimensionality reduction and manifold learning algorithms    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "### Feature engineering\n",
    "Feature engineering is the manual reshaping of our data to create new features.\n",
    "\n",
    "When we create new features we aim to improve the ability of our algorithms to model our data.\n",
    "\n",
    "Since our algorithms typically only find linear, quadratic, and recti-linear boundaries, we need to simplify complicated relationships into ones that can be modeled.\n",
    "\n",
    "Generating a new feature that is irrelevant will not help.\n",
    "\n",
    "Generating a new feature that is redundant with a previous one will not help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kernels\"></a>\n",
    "### Kernels\n",
    "\n",
    "Kernels are a different approach to non-linearity. \n",
    "\n",
    "Rather than transforming our data directly, we can transform the space that our data lives in.\n",
    "\n",
    "Common lernels are polynomial kernels and the **Radial Basis Function kernel** which allows use to transform linear relationships into polynomial ones or into gaussian blobs.\n",
    "\n",
    "Kernel methods are computationaly efficient and are prefered to doing the same kinds of transformations by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised-dimensionality-reduction-and-manifold-learning-algorithms\"></a>\n",
    "### Unsupervised dimensionality reduction and manifold learning algorithms\n",
    "\n",
    "Dimensionality reduction algorithms aim to remove redundancy from our data by recombining several feature into a smaller number of features.\n",
    "\n",
    "Primary Components Analysis is the quintessential \"dimensionality reduction\" algorithm. \n",
    "\n",
    "A related class of algorithm fall under the name of manifold learning, but they roughly the same focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a manifold?**\n",
    "\n",
    "**From wikipedia**: In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, each point of an n-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension n. In this more precise terminology, a manifold is referred to as an n-manifold.\n",
    "\n",
    "That is a very dense definition, it may be easier to digest using a map as an example.\n",
    "\n",
    "https://xkcd.com/977/\n",
    "\n",
    "When we make maps, we have to find a way to represent the Earth, a three dimensional object in 3-D euclidean space, in two dimensions. \n",
    "\n",
    "We can't just choose any way to draw the Earth in 2-D, we want to preserve a sense of distance so that every centimeter on the map corresponds to some number of kilometers in the real world. This is the \"locally resembles Euclidean space near each point\" part.\n",
    "\n",
    "Manifold learning is a form of dimensionality reduction that focuses on preserving particular relationships in the data.\n",
    "\n",
    "The key is to make sure that:\n",
    "- Every point in high-dimensional space corresponds to a point in low-dimensional space.\n",
    "- The distance between two points in high-dimensional space corresponds to the distance between the corresponding points in low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kernels-demo\"></a>\n",
    "## Kernels Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load in mammals data**\n",
    "\n",
    "We will be predicting brain weight using a mammal's body weight as we have done before. This time we will use kernels to fit a non-linear relationship without any explicit variable transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mammals = pd.read_csv('../../dataset/mammals.txt', sep='\\t')\n",
    "mammals = mammals.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammals.plot.scatter(y='Brain Weight', x='Body Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create X feature matrix and y outcome**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Body Weight']\n",
    "X = mammals[features]\n",
    "y = mammals['Brain Weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick an algorithm that has a built-in way to modify the kernel**\n",
    "\n",
    "In this case we will be using KernelRidge in sklearn: http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge\n",
    "\n",
    "It is not possible to modify kernels for all kinds of algorithms, but it is a frequent possibility.\n",
    "\n",
    "We use KernelRidge because you are familiar with linear regression and at least have a general sense of what ridge regression does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate  and inspect KernelRidge**\n",
    "\n",
    "The plot generate by this run shows the predictions of kernel ridge plotted against Body Weight.\n",
    "\n",
    "Try the following parameters and see how the predictions change\n",
    "\n",
    "kernel: 'linear'\n",
    "kernel: 'poly', degree: [1, 2]\n",
    "kernel: 'rbf', gamma: [0.00000001 ... 0.001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "kr = KernelRidge(kernel='linear')\n",
    "kr.fit(X, y)\n",
    "print mean_squared_error(y, kr.predict(X))\n",
    "plt.scatter(X['Body Weight'].values, kr.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dimensionality-reduction-demo\"></a>\n",
    "## Dimensionality Reduction Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction reduces the number of random variables that you are considering for analysis until you are left with the most important variables.\n",
    "\n",
    "Dimensionality reduction is not an end goal in itself, but a tool to form a dataset with more parsimonious features for further visualization and/or modelling.\n",
    "\n",
    "To get a quick summary of our data, we can calculate a covariance matrix, an unstandardized correlation matrix.\n",
    "\n",
    "The diagonal elements in a covariance matrix show us the variance of each of our features.\n",
    "\n",
    "The off-diagnal elements show the covariance, the amount of colinearity and redundancy between our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would an 'ideal' covariance matrix look like?**\n",
    "\n",
    "An \"ideal\" covariance matrix for data would have large numbers (variances) along the diagonal because this would indicate a large amount of potential signal in the data. It would also have zero values in the off-diagonal elements because these values indicate redundancy across our variables.\n",
    "What can we do to try to remove any redundancies and preserve the signal?\n",
    "Enter PCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA is the quintessential \"dimensionality reduction\" algorithm.**\n",
    "\n",
    "_Dimensionality reduction_ is the process of combining or collapsing the existing features (columns in X) into fewer features. \n",
    "\n",
    "These hopefully:\n",
    "\n",
    "- Retain the signal in the original data, and\n",
    "- Reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "PCA finds the linear combinations of your current predictor variables that create new \"principal components\". The principal components explain (in order) the maximum possible amount of variance in your predictors.\n",
    "\n",
    "A more natural way of thinking about PCA is that **it transforms the coordinate system so that the axes become the  most concise, informative descriptors of our data as a whole.**\n",
    "\n",
    "The old axes are the original variables (columns). The new axes are the principal components from PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-process-of-pca\"></a>\n",
    "### The Process of PCA \n",
    "\n",
    "---\n",
    "\n",
    "Say we have a matrix $X$ of predictor variables. PCA will give us the ability to transform our $X$ matrix into a new matrix $Z$. \n",
    "\n",
    "First we will derive a **weighting matrix** $W$ from the correlational/covariance structure of $X$ that allows us to perform the transformation.\n",
    "\n",
    "The full principal components decomposition of X can therefore be given as\n",
    "\n",
    "$${\\displaystyle \\mathbf {Z} =\\mathbf {X} \\mathbf {W} }$$\n",
    "\n",
    "Each successive dimension (column) in $Z$ will be rank-ordered according to variance in its values!\n",
    "\n",
    "**Two assumptions that PCA makes:**\n",
    "1. **Linearity:** The data does not hold nonlinear relationships.\n",
    "2. **Large variances define importance:** The dimensions are constructed to maximize remaining variance.\n",
    "\n",
    "The resulting principal components (columns of $Z$) will be uncorrelated. This makes PCA a useful preprocessing step for algorithms requiring uncorrelated input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"principal-components\"></a>\n",
    "### Principal Components\n",
    "\n",
    "---\n",
    "\n",
    "What is a principal component? **Principal components are the vectors that define the new coordinate system for your data.** Transforming your original data columns onto the principal component axes construct new variables that are optimized to explain as much variance as possible and to be independent (uncorrelated).\n",
    "\n",
    "Creating these variables is a well-defined mathematical process. In essence, **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-would-we-want-to-do-pca\"></a>\n",
    "### Why would we want to do PCA?\n",
    "\n",
    "---\n",
    "\n",
    "- We can reduce the number of dimensions (remove less important components), while losing mostly noise rather than signal.\n",
    "- Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n",
    "- The directions of largest variance should have the highest signal-to-noise ratio.\n",
    "- Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent.\n",
    "\n",
    "---\n",
    "\n",
    "[Good paper on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "[Nice site on performing PCA](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('fivethirtyeight') \n",
    "plt.rcParams['figure.figsize'] = (14,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the titanic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../dataset/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract a feature matrix, X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace missing values for age with the mean age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use StandardScaler from sklearn to normalize every feature for PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a scatterplot matrix of features. Color points by survivorship.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.scatter_matrix(pd.DataFrame(X, columns=features), c=df.Survived, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and transform a PCA decomposition using 2 components**\n",
    "\n",
    "Assign the resulting matrix to a variable called visualization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.scatter_matrix(pd.DataFrame(visualization_data), c=df.Survived, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the correlation matrix of visualization_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect the amount of variance that each primary component captures from the original feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect the component weightings**\n",
    "\n",
    "Do the weighting make sense?\n",
    "Do we seem to capture core representations of our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"other-applications\"></a>\n",
    "## Other Applications\n",
    "\n",
    "We used dimensionality reduction to help us explore our data, but there are many more ways to employ dimensionality reduction:\n",
    "- We can use our new feature matrix to make better predictions using superised learning\n",
    "    - This can be very powerful when we have lots of unlabeled data and some labeled data\n",
    "    - We can train dimensionality reduction on all of our data and use supervised learning on the labeled set which is now better represented.\n",
    "- Market Basket Analysis can be done by storing customers as rows and items purchased as columns\n",
    "- Latent Semantic Indexing using PCA (TruncatedSVD) to reduce redundant vocabulary in text problems\n",
    "- Word2Vec and Latent Dirichlet Allocation are two popular techniques that use similar principles to simpllify text data\n",
    "- Ultimately, dimensionality reduction leads us to neural networks which are capable of simultaneously learning more flexible decision boundaries and a better data representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
