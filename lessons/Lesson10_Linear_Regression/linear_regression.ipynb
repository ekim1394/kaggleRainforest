{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "_Authors: Kevin Markham (DC), Ed Podojil (NYC)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduce the bikeshare data](#introduce-the-bikeshare-data)\n",
    "\t- [Reading in the data](#reading-in-the-data)\n",
    "\t- [Visualizing the data](#visualizing-the-data)\n",
    "- [Linear regression basics](#linear-regression-basics)\n",
    "\t- [Form of linear regression](#form-of-linear-regression)\n",
    "\t- [Building a linear regression model](#building-a-linear-regression-model)\n",
    "\t- [Using the model for prediction](#using-the-model-for-prediction)\n",
    "\t- [Does the scale of the features matter?](#does-the-scale-of-the-features-matter)\n",
    "- [Working with multiple features](#working-with-multiple-features)\n",
    "\t- [Visualizing the data (part 2)](#visualizing-the-data-part-)\n",
    "\t- [Adding more features to the model](#adding-more-features-to-the-model)\n",
    "- [What is Multicollinearity?](#what-is-multicollinearity)\n",
    "- [Choosing between models](#choosing-between-models)\n",
    "\t- [Feature selection](#feature-selection)\n",
    "\t- [Evaluation metrics for regression problems](#evaluation-metrics-for-regression-problems)\n",
    "\t- [Comparing models with train/test split and RMSE](#comparing-models-with-traintest-split-and-rmse)\n",
    "\t- [Comparing testing RMSE with null RMSE](#comparing-testing-rmse-with-null-rmse)\n",
    "- [Creating features](#creating-features)\n",
    "\t- [Handling categorical features](#handling-categorical-features)\n",
    "\t- [Feature engineering](#feature-engineering)\n",
    "- [Regularization](#regularization)\n",
    "\t- [How does regularization work?](#how-does-regularization-work)\n",
    "\t- [Lasso and ridge path diagrams](#lasso-and-ridge-path-diagrams)\n",
    "\t- [Advice for applying regularization](#advice-for-applying-regularization)\n",
    "\t- [Ridge regression](#ridge-regression)\n",
    "- [Comparing linear regression with other models](#comparing-linear-regression-with-other-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduce-the-bikeshare-data\"></a>\n",
    "## Introduce the bikeshare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-in-the-data\"></a>\n",
    "<a id=\"reading-in-the-data\"></a>\n",
    "### Reading in the data\n",
    "\n",
    "We'll be working with a dataset from Capital Bikeshare that was used in a Kaggle competition ([data dictionary](https://www.kaggle.com/c/bike-sharing-demand/data))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data and set the datetime as the index\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/bikeshare.csv'\n",
    "bikes = pd.read_csv(url, index_col='datetime', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01 00:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 01:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 02:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 03:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 04:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     season  holiday  workingday  weather  temp   atemp  \\\n",
       "datetime                                                                  \n",
       "2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "                     humidity  windspeed  casual  registered  count  \n",
       "datetime                                                             \n",
       "2011-01-01 00:00:00        81        0.0       3          13     16  \n",
       "2011-01-01 01:00:00        80        0.0       8          32     40  \n",
       "2011-01-01 02:00:00        80        0.0       5          27     32  \n",
       "2011-01-01 03:00:00        75        0.0       3          10     13  \n",
       "2011-01-01 04:00:00        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- What does each observation represent?\n",
    "- What is the response variable (as defined by Kaggle)?\n",
    "- How many features are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"count\" is a method, so it's best to name that column something else\n",
    "bikes.rename(columns={'count':'total'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data\"></a>\n",
    "<a id=\"visualizing-the-data\"></a>\n",
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas scatter plot\n",
    "bikes.plot(kind='scatter', x='temp', y='total', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seaborn scatter plot with regression line\n",
    "sns.lmplot(x='temp', y='total', data=bikes, aspect=1.5, scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-regression-basics\"></a>\n",
    "## Linear regression basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"form-of-linear-regression\"></a>\n",
    "<a id=\"form-of-linear-regression\"></a>\n",
    "### Form of linear regression\n",
    "\n",
    "$y = \\alpha + \\beta X + \\epsilon$\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$\n",
    "\n",
    "- $y$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**:\n",
    "\n",
    "- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n",
    "- Specifically, we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "- And once we've learned these coefficients, we can use the model to predict the response.\n",
    "\n",
    "![Estimating coefficients](./assets/images/estimating_coefficients.png)\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"building-a-linear-regression-model\"></a>\n",
    "<a id=\"building-a-linear-regression-model\"></a>\n",
    "### Building a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['temp']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import, instantiate, fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print linreg.intercept_\n",
    "print linreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the **intercept** ($\\beta_0$):\n",
    "\n",
    "- It is the value of $y$ when $x$=0.\n",
    "- Thus, it is the estimated number of rentals when the temperature is 0 degrees Celsius.\n",
    "- **Note:** It does not always make sense to interpret the intercept. (Why?)\n",
    "\n",
    "Interpreting the **\"temp\" coefficient** ($\\beta_1$):\n",
    "\n",
    "- It is the change in $y$ divided by change in $x$, or the \"slope\".\n",
    "- Thus, a temperature increase of 1 degree Celsius is **associated with** a rental increase of 9.17 bikes.\n",
    "- This is not a statement of causation.\n",
    "- $\\beta_1$ would be **negative** if an increase in temperature was associated with a **decrease** in rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-the-model-for-prediction\"></a>\n",
    "<a id=\"using-the-model-for-prediction\"></a>\n",
    "### Using the model for prediction\n",
    "\n",
    "How many bike rentals would we predict if the temperature was 25 degrees Celsius?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually calculate the prediction\n",
    "linreg.intercept_ + linreg.coef_*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the predict method\n",
    "linreg.predict(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"does-the-scale-of-the-features-matter\"></a>\n",
    "<a id=\"does-the-scale-of-the-features-matter\"></a>\n",
    "### Does the scale of the features matter?\n",
    "\n",
    "Let's say that temperature was measured in Fahrenheit, rather than Celsius. How would that affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column for Fahrenheit temperature\n",
    "bikes['temp_F'] = bikes.temp * 1.8 + 32\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seaborn scatter plot with regression line\n",
    "sns.lmplot(x='temp_F', y='total', data=bikes, aspect=1.5, scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['temp_F']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print linreg.intercept_\n",
    "print linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert 25 degrees Celsius to Fahrenheit\n",
    "25 * 1.8 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict rentals for 77 degrees Fahrenheit\n",
    "linreg.predict(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The scale of the features is **irrelevant** for linear regression models. When changing the scale, we simply change our **interpretation** of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the temp_F column\n",
    "bikes.drop('temp_F', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"working-with-multiple-features\"></a>\n",
    "## Working with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data-part-\"></a>\n",
    "<a id=\"visualizing-the-data-part-\"></a>\n",
    "### Visualizing the data (part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore more features\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiple scatter plots in Seaborn\n",
    "sns.pairplot(bikes, x_vars=feature_cols, y_vars='total', kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiple scatter plots in Pandas\n",
    "fig, axs = plt.subplots(1, len(feature_cols), sharey=True)\n",
    "for index, feature in enumerate(feature_cols):\n",
    "    bikes.plot(kind='scatter', x=feature, y='total', ax=axs[index], figsize=(16, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you seeing anything that you did not expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-tabulation of season and month\n",
    "pd.crosstab(bikes.season, bikes.index.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# box plot of rentals, grouped by season\n",
    "bikes.boxplot(column='total', by='season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably:\n",
    "\n",
    "- A line can't capture a non-linear relationship.\n",
    "- There are more rentals in winter than in spring (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# line plot of rentals\n",
    "bikes.total.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us?\n",
    "\n",
    "There are more rentals in the winter than the spring, but only because the system is experiencing **overall growth** and the winter months happen to come after the spring months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correlation matrix (ranges from 1 to -1)\n",
    "bikes.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(bikes.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What relationships do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"adding-more-features-to-the-model\"></a>\n",
    "<a id=\"adding-more-features-to-the-model\"></a>\n",
    "### Adding more features to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, one variable explained the variance of another; however, more often than not, we will need multiple variables. \n",
    "\n",
    "- For example, a house's price may be best measured by square feet, but a lot of other variables play a vital role: bedrooms, bathrooms, location, appliances, etc. \n",
    "\n",
    "- For a linear regression, we want these variables to be largely independent of each other, but all of them should help explain the y variable.\n",
    "\n",
    "We'll work with bike-share data to showcase what this means and to explain a concept called multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of features\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print linreg.intercept_\n",
    "print linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pair the feature names with the coefficients\n",
    "zip(feature_cols, linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "- Holding all other features fixed, a 1 unit increase in **temperature** is associated with a **rental increase of 7.86 bikes**.\n",
    "- Holding all other features fixed, a 1 unit increase in **season** is associated with a **rental increase of 22.5 bikes**.\n",
    "- Holding all other features fixed, a 1 unit increase in **weather** is associated with a **rental increase of 6.67 bikes**.\n",
    "- Holding all other features fixed, a 1 unit increase in **humidity** is associated with a **rental decrease of 3.12 bikes**.\n",
    "\n",
    "Does anything look incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-multicollinearity\"></a>\n",
    "<a id=\"what-is-multicollinearity\"></a>\n",
    "## What is Multicollinearity?\n",
    "\n",
    "With the bike share data, let's compare three data points: actual temperature, \"feel\" temperature, and guest ridership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "correlations = bikes[['temp', 'atemp', 'casual']].corr()\n",
    "print correlations\n",
    "print sns.heatmap(correlations, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of features\n",
    "feature_cols = ['temp', 'atemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print linreg.intercept_\n",
    "print linreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"choosing-between-models\"></a>\n",
    "## Choosing between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-selection\"></a>\n",
    "<a id=\"feature-selection\"></a>\n",
    "### Feature selection\n",
    "\n",
    "How do we choose which features to include in the model? We're going to use **train/test split** (and eventually **cross-validation**).\n",
    "\n",
    "Why not use of **p-values** or **R-squared** for feature selection?\n",
    "\n",
    "- Linear models rely upon **a lot of assumptions** (such as the features being independent), and if those assumptions are violated, p-values and R-squared are less reliable. Train/test split relies on fewer assumptions.\n",
    "- Features that are unrelated to the response can still have **significant p-values**.\n",
    "- Adding features to your model that are unrelated to the response will always **increase the R-squared value**, and adjusted R-squared does not sufficiently account for this.\n",
    "- p-values and R-squared are **proxies** for our goal of generalization, whereas train/test split and cross-validation attempt to **directly estimate** how well the model will generalize to out-of-sample data.\n",
    "\n",
    "More generally:\n",
    "\n",
    "- There are different methodologies that can be used for solving any given data science problem, and this course follows a **machine learning methodology**.\n",
    "- This course focuses on **general purpose approaches** that can be applied to any model, rather than model-specific approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-metrics-for-regression-problems\"></a>\n",
    "<a id=\"evaluation-metrics-for-regression-problems\"></a>\n",
    "### Evaluation metrics for regression problems\n",
    "\n",
    "Evaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. We need evaluation metrics designed for comparing **continuous values**.\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example true and predicted response values\n",
    "true = [10, 7, 5, 5]\n",
    "pred = [8, 6, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print 'MAE:', metrics.mean_absolute_error(true, pred)\n",
    "print 'MSE:', metrics.mean_squared_error(true, pred)\n",
    "print 'RMSE:', np.sqrt(metrics.mean_squared_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these metrics:\n",
    "\n",
    "- **MAE** is the easiest to understand, because it's the average error.\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are **loss functions**, because we want to minimize them.\n",
    "\n",
    "Here's an additional example, to demonstrate how MSE/RMSE punish larger errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same true values as above\n",
    "true = [10, 7, 5, 5]\n",
    "\n",
    "# new set of predicted values\n",
    "pred = [10, 7, 5, 13]\n",
    "\n",
    "# MAE is the same as before\n",
    "print 'MAE:', metrics.mean_absolute_error(true, pred)\n",
    "\n",
    "# MSE and RMSE are larger than before\n",
    "print 'MSE:', metrics.mean_squared_error(true, pred)\n",
    "print 'RMSE:', np.sqrt(metrics.mean_squared_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-models-with-traintest-split-and-rmse\"></a>\n",
    "<a id=\"comparing-models-with-traintest-split-and-rmse\"></a>\n",
    "### Comparing models with train/test split and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define a function that accepts a list of features and returns testing RMSE\n",
    "def train_test_rmse(feature_cols):\n",
    "    X = bikes[feature_cols]\n",
    "    y = bikes.total\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_test)\n",
    "    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare different sets of features\n",
    "print train_test_rmse(['temp', 'season', 'weather', 'humidity'])\n",
    "print train_test_rmse(['temp', 'season', 'weather'])\n",
    "print train_test_rmse(['temp', 'season', 'humidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using these as features is not allowed!\n",
    "print train_test_rmse(['casual', 'registered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-testing-rmse-with-null-rmse\"></a>\n",
    "<a id=\"comparing-testing-rmse-with-null-rmse\"></a>\n",
    "### Comparing testing RMSE with null RMSE\n",
    "\n",
    "Null RMSE is the RMSE that could be achieved by **always predicting the mean response value**. It is a benchmark against which you may want to measure your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "# create a NumPy array with the same shape as y_test\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "# fill the array with the mean value of y_test\n",
    "y_null.fill(y_test.mean())\n",
    "y_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute null RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"creating-features\"></a>\n",
    "## Creating features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"handling-categorical-features\"></a>\n",
    "<a id=\"handling-categorical-features\"></a>\n",
    "### Handling categorical features\n",
    "\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "\n",
    "- **Ordered categories:** transform them to sensible numeric values (example: small=1, medium=2, large=3)\n",
    "- **Unordered categories:** use dummy encoding (0/1)\n",
    "\n",
    "What are the categorical features in our dataset?\n",
    "\n",
    "- **Ordered categories:** weather (already encoded with sensible numeric values)\n",
    "- **Unordered categories:** season (needs dummy encoding), holiday (already dummy encoded), workingday (already dummy encoded)\n",
    "\n",
    "For season, we can't simply leave the encoding as 1 = spring, 2 = summer, 3 = fall, and 4 = winter, because that would imply an **ordered relationship**. Instead, we create **multiple dummy variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "season_dummies = pd.get_dummies(bikes.season, prefix='season')\n",
    "\n",
    "# print 5 random rows\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we actually only need **three dummy variables (not four)**, and thus we'll drop the first dummy variable.\n",
    "\n",
    "Why? Because three dummies captures all of the \"information\" about the season feature, and implicitly defines spring (season 1) as the **baseline level:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the first column\n",
    "season_dummies.drop(season_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# print 5 random rows\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if you have a categorical feature with **k possible values**, you create **k-1 dummy variables**.\n",
    "\n",
    "If that's confusing, think about why we only need one dummy variable for holiday, not two dummy variables (holiday_yes and holiday_no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns)\n",
    "bikes = pd.concat([bikes, season_dummies], axis=1)\n",
    "\n",
    "# print 5 random rows\n",
    "bikes.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include dummy variables for season in the model\n",
    "feature_cols = ['temp', 'season_2', 'season_3', 'season_4', 'humidity']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "zip(feature_cols, linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the season coefficients? They are **measured against the baseline (spring)**:\n",
    "\n",
    "- Holding all other features fixed, **summer** is associated with a **rental decrease of 3.39 bikes** compared to the spring.\n",
    "- Holding all other features fixed, **fall** is associated with a **rental decrease of 41.7 bikes** compared to the spring.\n",
    "- Holding all other features fixed, **winter** is associated with a **rental increase of 64.4 bikes** compared to the spring.\n",
    "\n",
    "Would it matter if we changed which season was defined as the baseline?\n",
    "\n",
    "- No, it would simply change our **interpretation** of the coefficients.\n",
    "\n",
    "**Important:** Dummy encoding is relevant for all machine learning models, not just linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare original season variable with dummy variables\n",
    "print train_test_rmse(['temp', 'season', 'humidity'])\n",
    "print train_test_rmse(['temp', 'season_2', 'season_3', 'season_4', 'humidity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "<a id=\"feature-engineering\"></a>\n",
    "### Feature engineering\n",
    "\n",
    "See if you can create the following features:\n",
    "\n",
    "- **hour:** as a single numeric feature (0 through 23)\n",
    "- **hour:** as a categorical feature (use 23 dummy variables)\n",
    "- **daytime:** as a single categorical feature (daytime=1 from 7am to 8pm, and daytime=0 otherwise)\n",
    "\n",
    "Then, try using each of the three features (on its own) with `train_test_rmse` to see which one performs the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hour as a numeric feature\n",
    "bikes['hour'] = bikes.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hour as a categorical feature\n",
    "hour_dummies = pd.get_dummies(bikes.hour, prefix='hour')\n",
    "hour_dummies.drop(hour_dummies.columns[0], axis=1, inplace=True)\n",
    "bikes = pd.concat([bikes, hour_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# daytime as a categorical feature\n",
    "bikes['daytime'] = ((bikes.hour > 6) & (bikes.hour < 21)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print train_test_rmse(['hour'])\n",
    "print train_test_rmse(bikes.columns[bikes.columns.str.startswith('hour_')])\n",
    "print train_test_rmse(['daytime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"regularization\"></a>\n",
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization is a method for \"constraining\" or \"regularizing\" the **size of the coefficients**, thus \"shrinking\" them towards zero.\n",
    "- It reduces model variance and thus **minimizes overfitting**.\n",
    "- If the model is too complex, it tends to reduce variance more than it increases bias, resulting in a model that is **more likely to generalize**.\n",
    "\n",
    "Our goal is to locate the **optimum model complexity**, and thus regularization is useful when we believe our model is too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-does-regularization-work\"></a>\n",
    "<a id=\"how-does-regularization-work\"></a>\n",
    "### How does regularization work?\n",
    "\n",
    "For a normal linear regression model, we estimate the coefficients using the least squares criterion, which **minimizes the residual sum of squares (RSS):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a regularized linear regression model, we **minimize the sum of RSS and a \"penalty term\"** that penalizes coefficient size.\n",
    "\n",
    "**Ridge regression** (or \"L2 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "**Lasso regression** (or \"L1 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "- $p$ is the **number of features**\n",
    "- $\\beta_j$ is a **model coefficient**\n",
    "- $\\alpha$ is a **tuning parameter:**\n",
    "    - A tiny $\\alpha$ imposes no penalty on the coefficient size, and is equivalent to a normal linear regression model.\n",
    "    - Increasing the $\\alpha$ penalizes the coefficients and thus shrinks them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lasso-and-ridge-path-diagrams\"></a>\n",
    "<a id=\"lasso-and-ridge-path-diagrams\"></a>\n",
    "### Lasso and ridge path diagrams\n",
    "\n",
    "A larger alpha (towards the left of each diagram) results in more regularization:\n",
    "\n",
    "- **Lasso regression** shrinks coefficients all the way to zero, thus removing them from the model\n",
    "- **Ridge regression** shrinks coefficients toward zero, but they rarely reach zero\n",
    "\n",
    "Source code for the diagrams: [Lasso regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html) and [Ridge regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lasso and Ridge Coefficient Plots](./assets/images/lasso_ridge_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advice-for-applying-regularization\"></a>\n",
    "<a id=\"advice-for-applying-regularization\"></a>\n",
    "### Advice for applying regularization\n",
    "\n",
    "**Should features be standardized?**\n",
    "\n",
    "- Yes, because otherwise, features would be penalized simply because of their scale.\n",
    "- Also, standardizing avoids penalizing the intercept, which wouldn't make intuitive sense.\n",
    "\n",
    "**How should you choose between Lasso regression and Ridge regression?**\n",
    "\n",
    "- Lasso regression is preferred if we believe many features are irrelevant or if we prefer a sparse model.\n",
    "- Ridge can work particularly well if there is a high degree of multicolinearity in your model.\n",
    "- If model performance is your primary concern, it is best to try both.\n",
    "- ElasticNet regression is a combination of lasso regression and ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ridge-regression\"></a>\n",
    "<a id=\"ridge-regression\"></a>\n",
    "### Ridge regression\n",
    "\n",
    "- [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) documentation\n",
    "- **alpha:** must be positive, increase for more regularization\n",
    "- **normalize:** scales the features (without using StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include dummy variables for season in the model\n",
    "feature_cols = ['temp', 'atemp', 'season_2', 'season_3', 'season_4', 'humidity']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alpha=0 is equivalent to linear regression\n",
    "from sklearn.linear_model import Ridge\n",
    "ridgereg = Ridge(alpha=0, normalize=True)\n",
    "ridgereg.fit(X_train, y_train)\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip(feature_cols, ridgereg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret these coefficients we need to convert them back to original units, which is a reason to do normalization by hand. However, in this form the coefficients have a special meaning. The intercept is now the average of our outcome, and the magnitude of each coefficient in the model is a measure of how important it is in the model. We call this feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try alpha=0.1\n",
    "ridgereg = Ridge(alpha=0.1, normalize=True)\n",
    "ridgereg.fit(X_train, y_train)\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# examine the coefficients\n",
    "zip(feature_cols, ridgereg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-linear-regression-with-other-models\"></a>\n",
    "<a id=\"comparing-linear-regression-with-other-models\"></a>\n",
    "## Comparing linear regression with other models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain\n",
    "- Highly interpretable\n",
    "- Model training and prediction are fast\n",
    "- No tuning is required (excluding regularization)\n",
    "- Features don't need scaling\n",
    "- Can perform well with a small number of observations\n",
    "- Well-understood\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "- Can't automatically learn feature interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
